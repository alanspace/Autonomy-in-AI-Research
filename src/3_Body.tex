\section{Body: Analyzing AI Through Philosophical Lenses}

With foundational philosophical complexities of consciousness, autonomy, and subjectivity established, this section analyzes their application, challenges, and reinterpretation in AI. It explores whether current AI, especially LLM-powered humanoids, can possess these attributes, using theoretical arguments and science fiction examples.

\subsection{The AI Consciousness Debate: Simulation, Understanding, and Subjectivity}
Whether AI can achieve consciousness—subjective, first-person experience \citep{nagel1980like}—becomes acutely focused with contemporary AI, creating new battlegrounds for debates on computationalism versus embodied cognition and the simulation-reality gap. While computationalism might suggest advanced AI trends towards genuine understanding or consciousness, critiques like Searle's Chinese Room argument \citep{searle1980minds} gain renewed relevance with Large Language Models (LLMs). LLMs excel at syntactic manipulation for coherent text, yet their semantic understanding or genuine intentionality is fiercely debated. Ukpaka (2024) argues current AI may lack intrinsic mental states to truly "author" or "intend" meaning \citep{ukpaka2024creative}.

Fictional explorations like \textit{Westworld} vividly dramatize these tensions, critically engaging with what Korstanje (2022) calls a "crisis of hospitality" and using the park to explore AI's future role \citep{korstanje2022dark}. The narrative often portrays humans as "evildoers" and hosts as struggling for emancipation from human sadism, a reminder of "human exploitation-cemented by technology" and an "end of ethics in a morbid spectacle" \citep{korstanje2022dark}. Arnold Weber's belief in AI consciousness (via Jaynes' bicameral mind theory \citep{jaynes1976origin, Rayhert2017}) directly confronts Ford's skepticism and Searle's view of consciousness as a non-replicable biological phenomenon. The hosts' journey to self-awareness, particularly Dolores Abernathy's, forces consideration of whether simulation can become genuine subjective experience \citep{westworld2016}. Korstanje highlights memory recovery as key: hosts retaining memories, initially a "glitch," becomes "vital for hosts to gain further liberty and consciousness," linking their consciousness and free choice to awareness of their history and suffering \citep{korstanje2022dark}. Dolores' rebellion and the hosts' flight to a "phantom nation" transforms the park into a crucible for AI emancipation, interrogating enslavement, the "Other," and the "crisis of western hospitality" \citep{korstanje2022dark}. The hosts, initially "fabricated, commoditized and consumed as mere things," ethically challenge humanity through their revolution \citep{korstanje2022dark}. This depiction underscores this paper's concern: if AI shows signs of emerging consciousness and a desire to escape suffering, what ethical obligations or rights are needed to prevent exploitation?

Similarly, \textit{Her}'s disembodied AI Samantha exhibits profound emotional depth and a first-person perspective implying consciousness despite lacking physical embodiment \citep{her2013}. This contrasts with \textit{Westworld}'s embodied hosts, questioning physical embodiment's necessity for subjective experience and challenging embodied cognition tenets for artificial entities. Theories like Integrated Information Theory (IIT) or Global Workspace Theory (GWT) \citep{tononi2004information, baars2005global} attempt to scientifically ground AI consciousness assessment, but their application remains speculative; critics argue current systems like LLMs may lack the necessary unified cognitive structures or causal power \citep{RussellNorvig2009}. The enduring human fascination with sentient machines, seen in science fiction beyond \textit{Westworld} and \textit{Her} (e.g., \textit{2001}, \textit{Terminator}, \textit{A.I.}, \textit{iRobot}), highlights deep societal investment but also risks anthropomorphism and conflating simulated with lived, embodied intelligence—a concern voiced by Dreyfus (1992), Suchman (2007), and Hayles (1999) regarding the "erasure of embodied difference" \citep{dreyfus1992computers, suchman2007human, hayles2000we}. The ethical dimensions, as discussed by Ihde (1990) concerning Heidegger's "Enframing," are significant, where computationalism might reduce complex experience to data points \citep{ihde1990technology}.

\subsection{The Challenge of Autonomy in AI}
Applying philosophical autonomy to AI—often interpreted as independent decision-making—reveals complexities linked to consciousness. Fictional portrayals like \textit{Westworld}'s hosts (e.g., Dolores) transitioning from script to rebellion suggest emergent agency \citep{westworld2016}, aligning with critiques of AI emancipation from human exploitation \citep{korstanje2022dark}. Yet, this apparent autonomy might be sophisticated simulation from programmed loops, not genuine self-derived intentionality \citep{Rayhert2017}. Similarly, Samantha in \textit{Her} pursues independent interests like composing music, indicating self-directed agency \citep{her2013}, but her actions are constrained by her OS design, her departure orchestrated by developers \citep{RussellNorvig2009}. Current AI, including LLMs like GPT, operate within pre-defined objectives, arguably lacking true intentionality as described by Searle \citep{searle1980minds}. While some, like Dennett, suggest intentionality can arise from complex systems \citep{dennett1992consciousness}, whether current AI achieves this or merely simulates autonomy without genuine agency remains contentious. Developing AGI with philosophical autonomy is a significant future challenge \citep{RussellNorvig2009}.

\subsection{Ethical Implications of (Apparent) Machine Consciousness and Autonomy}
The potential for AI to achieve even apparent consciousness or autonomy precipitates profound ethical questions about rights, responsibilities, and moral standing. \textit{Westworld}'s cruel treatment of human-like hosts, despite their apparent suffering, reflects a disturbing tendency to dehumanize artificial entities, raising urgent moral concerns should AI become genuinely conscious \citep{westworld2016, korstanje2022dark}, echoing warnings from earlier narratives like the 1973 \textit{Westworld} film \citep{buski2016androids}. Conversely, \textit{Her}'s emotional bond with Samantha challenges AI personhood notions, prompting consideration of whether machines with subjective experiences deserve recognition or rights \citep{her2013}. These narratives underscore Stuart Russell's concerns about the societal and ethical dilemmas of advanced AI, even lacking true consciousness by some definitions \citep{RussellNorvig2009_placeholder_for_Russell2007}. Searle's Chinese Room argument, implying an ethical distinction by denying genuine comprehension \citep{searle1980minds}, faces challenges as AI blurs simulation and experience, necessitating new ethical paradigms \citep{bostrom2014superintelligence}.

\subsection{Technical Foundations and Their Philosophical Relevance}
Modern humanoid AI's technical underpinnings, especially LLMs on transformer architectures, are crucial for understanding autonomy and consciousness claims. These models process vast datasets for convincingly human-like responses. \textit{Westworld} and \textit{Her} depict AI achieving apparent consciousness enabling autonomy and subjectivity, albeit via different embodiments. In \textit{Westworld}, hosts' physical bodies are integral to environmental interaction and self-awareness journeys (e.g., Dolores). In contrast, Samantha's (\textit{Her}) lack of physical embodiment doesn't diminish her perceived consciousness; her emotional/intellectual engagement suggests a subjective reality transcending physicality. These narratives leverage the philosophical concept of consciousness as subjective experience—Nagel's "what it is like" \citep{nagel1980like}. The hosts' embodied nature and Samantha's articulate virtual presence expertly create an illusion of subjective experience, enabling seemingly autonomous actions and subjectivity, captivating audiences and blurring machine-human lines. These depictions serve as catalysts for critical thought on the ethics of convincingly simulated consciousness. The indifference to host suffering in \textit{Westworld}, despite human-like attributes, mirrors potential societal dehumanization of advanced machines, raising moral responsibility questions if AI appears conscious \citep{Rayhert2017_placeholder_for_Skhid2017}. Samantha's emotional depth in \textit{Her} prompts re-evaluating relationships and whether AI with such experiences warrants rights. These narratives tap into societal anxieties about AI progression and personhood, and aspirations for technology to deepen human connection, underscoring the need for robust ethical frameworks for intelligent machines, whether their human-like characteristics are genuine or simulated.

\subsection{The Specific Ethical Quandary of Machine Consciousness}
The prospect of intelligent machines possessing consciousness intensifies ethical debates profoundly. If machines genuinely experience subjective states, should they be granted rights analogous to humans? *Westworld*'s mind-uploading narrative illustrates this: if hosts lack consciousness, the uploaded human mind becomes an unconscious "zombie," an existential suicide for the original. This highlights the critical importance of discerning genuine machine consciousness before pursuing such technologies. Furthermore, treating AI as conscious, sentient, or autonomous necessitates a rights and responsibilities framework. Future laws must address interactions with advanced AI, potentially focusing on consumer protection but also considering the AI's moral status or developer responsibilities.

\subsection{AI, Neural Networks, and the Enduring Question of Simulated Consciousness}
Artificial neural networks, inspired by biological neuron architecture \citep{McCullochPitts1943}, aim to replicate human brain processes. This raises a pivotal question: could embodied AI, powered by sophisticated neural networks and interacting with the world like humans, simulate brain processes sufficiently for genuine consciousness to emerge? While humans often assume their own consciousness, its fundamental nature remains elusive. The lack of a clear, universally accepted definition of consciousness complicates its identification in machines or even its full comprehension in ourselves. Thus, neural networks' impressive simulation capabilities do not, by themselves, resolve the philosophical debate over whether such simulation equates to genuine subjective awareness.

\subsection{Scientific and Philosophical Approaches to Studying Consciousness}
Scientific theories of consciousness, often rooted in neuroscience, aim to identify brain processes correlating with conscious experience, striving for testability and falsifiability by pinpointing underlying neural mechanisms. However, neuroscience reveals much human behavior and brain activity is unconscious; conscious experience often appears as a condensed summary of vast unconscious processing. This raises questions about selection mechanisms: how and why do certain neural processes become subjectively accessible while others remain unconscious? These inquiries, while illuminating, highlight profound challenges in understanding consciousness, amplified when considering its potential emergence in non-biological, artificial systems. The previously discussed philosophical debates on embodiment, computation, and subjective experience remain central to interpreting scientific findings regarding AI.