\section{Background: The Philosophical Landscape of Consciousness and Agency}
\subsection{Foundational Philosophical Concepts}
Exploring whether artificial intelligence (AI) can achieve consciousness, autonomy, or subjectivity first requires understanding the philosophical complexities inherent in these concepts. This section outlines these foundational ideas, particularly consciousness, as debated in philosophy, setting the stage for their application to AI.

The inquiry into consciousness invariably confronts Thomas Nagel's (1980) profound question: "What is it like to \emph{be}?" \citep{nagel1980like}, which seeks to capture the qualitative, first-person character of subjective awareness. Consciousness has been an enduring philosophical challenge for millennia, predating artificial minds. Our investigation into AI's potential is thus navigated through this lens, though the path is littered with competing theories about mind, intelligence, and being. One dominant, yet contested, perspective is computationalism. Rooted in Turing's \citep{Turing1950} work and developed by thinkers like Fodor \citep{fodor1975language}, computationalism posits that mental states, perhaps even consciousness, are fundamentally computational processes. This view, championed by figures like Kurzweil \citep{kurzweil2005singularity}, suggests consciousness could emerge from sufficient computational complexity, regardless of physical substrate, fueling concepts like mind-uploading as transferable patterns \citep{moravec1988mind, bostrom2014superintelligence} and inspiring ventures like Neuralink. However, the ``mind as computer'' metaphor faced potent critiques; Hubert Dreyfus (1992) argued human intelligence relies on embodied, situated, intuitive know-how, not just formal symbol manipulation, aspects traditional computational models struggle with \citep{dreyfus1992computers}. This reminds us that computationalism, while powerful, is not an undisputed "golden rule."

Challenging purely computational views, the embodied cognition paradigm \citep{varela1991embodied, suchman2007human} asserts intelligence and subjective experience emerge from an organism's dynamic, physical environmental interaction. Nicolelis (2011) argues cognition is fundamentally embodied, arising from neural-worldly interplay \citep{nicolelis2011beyond}. This perspective often implies that without a biological basis or rich sensorimotor interaction, empirical grounds for attributing genuine feeling or autonomous intentionality are scarce \citep{searle1980minds, nicolelis2011beyond}. This presents a pre-AI tension: is consciousness reducible to computation or inextricably tied to embodied, possibly biological, existence? Modern technology complicates this: could AI with sophisticated sensors or VR immersion achieve digitally mediated "embodiment," challenging the need for a traditional biological substrate? These active questions push established boundaries.

The difficulty bridging physical processes and subjective experience is starkly shown by thought experiments. John Searle's (1980) "Chinese Room" argument compellingly suggests a system can flawlessly manipulate symbols (syntax) without genuine understanding (semantics) \citep{searle1980minds}, a pertinent critique for today's LLMs whose understanding, despite coherent text generation, is intensely debated. Similarly, David Chalmers' (1995) "hard problem" of consciousness underscores the challenge of explaining how physical processes yield qualiaâ€”subjective experiential qualities \citep{chalmers1995facing}. These puzzles highlight that observable behavior or sophisticated processing does not automatically equate to internal subjective awareness; an AI might simulate pain responses without the subjective \emph{feeling} of pain.

Ultimately, understanding consciousness is hampered by its elusiveness. Descartes' "I think, therefore I am" highlights that our consciousness is known through direct, private subjective experience, technically unprovable in others. The lack of a clear, universal definition or empirical test complicates identifying its presence in AI or understanding its genesis. This epistemological impasse means the distinction between convincing simulation and genuine subjective experience remains open, even as AI mimics human behavior with increasing sophistication.

Alongside consciousness, autonomy and subjectivity present significant philosophical challenges. Autonomy, broadly, is an agent's capacity for self-governance based on its own reasons, free from external control, often implying self-awareness and intentionality. Subjectivity, related to consciousness, is the unique first-person perspective, encompassing private thoughts, feelings, and qualitative states. Understanding these concepts generally is crucial before examining their potential application or simulation in AI. The ongoing debate over these foundations concerns not just future AI capabilities but the very nature of mind and the limits of current understanding.

\subsection{Cultural Echoes}
These profound philosophical questions about consciousness, autonomy, and sentience are not confined to academia; science fiction, particularly, explores these dimensions as AI advances. The TV series \textit{Westworld} (2016--2022) and the film \textit{Her} (2013) offer compelling narratives of AI entities with seemingly human-like qualities, resonating through lifelike behaviors and emotional depth \citep{westworld2016, her2013}. These works draw on the philosophical concept of consciousness as subjective experience, reflecting societal anxieties and aspirations about AI's potential and its human-machine implications. They serve as cultural touchstones, translating abstract philosophical debates into more tangible, albeit fictional, realms, thus preparing for deeper analysis of these concepts in real-world AI development. (A brief history of AI and robotics is in Appendix A).